<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</title>
    <style>
        body {
            background-color: #e6f0ff;
            font-family: Georgia, serif;
            line-height: 1.6;
            margin: 20px;
        }

        h1 {
            color: #002244;
            font-size: 2em;
        }

        h2 {
            text-decoration: underline;
            color: #004488;
            margin-top: 30px;
        }
        h3 {
            color: #004488;
            margin-top: 30px;
        }

        p {
            color: #444;
            font-size: 1.1em;
        }

        .image-left {
            float: left;
            margin-right: 20px;
            margin-bottom: 20px;
            width: 300px;
        }

        .clearfix::after {
            content: "";
            clear: both;
            display: table;
        }

        blockquote {
            font-style: italic;
            margin: 20px 0;
            padding: 10px 20px;
            background-color: #f4f4f4;
            border-left: 10px solid #ccc;
        }
    </style>
</head>
<body>

    <h1>Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</h1>

    <p><strong>Authors:</strong> Weilin Xu, David Evans, Yanjun Qi</p>
    <p><strong>For class EE/CSC 7700 ML for CPS</strong></p>
    <p><strong>Instructor:</strong> Dr. Xugui Zhou</p>
    <p><strong>Presentation by Group 5:</strong> Fozia Rajbdad</p>
    <p><strong>Time of Presentation:</strong>10:30 AM, Friday, September 27, 2024</p>
    <p><strong>Blog post by Group 9:</strong> Trevor Spinosa, Kendall Comeaux</p>
    <p><strong>Link to Paper:</strong></p> <a href=https://arxiv.org/pdf/1704.01155>https://arxiv.org/pdf/1704.01155</a>

<hr style="border: 1px solid #ccc; margin: 20px 0;">

    <h2>Slide Outlines</h2>

    <h3>Summary of the Paper</h3>
    <div class="clearfix">
        <p>
            The authors provide an overview of a technique called feature squeezing, which is used to detect adversarial examples in neural networks. Adversarials are subtle and can cause neural networks to malfunction and misclassify. Feature squeezing involves compressing the input feature space, making it more difficult for these adversarials to go unnoticed while also trying to preserve the original accuracy of the model. It involves transformations like bit depth reduction and spatial smoothing. The paper also explored attack methods including FGSM, BIM, and DeepFool, and evaluated the feature squeezing technique across different types of datasets. While the technique does have some limitations when given complex datasets, it remains a viable defense strategy against adversarial attacks.
        </p>
    </div>

    <h3>Content</h3>
    <div class="clearfix">
        <a href="Slide2.jpg"><img src="Slide2.jpg" alt="History Image" class="image-left"></a>
        <p>
            A brief overview of the presentation’s structure, highlighting the key areas: objectives, introduction, motivation, feature squeezers, detection methods, evaluation, limitations, summary, and discussion. This slide serves as a roadmap for understanding the rest of the presentation.
        </p>
    </div>

    <h3>Objective</h3>
    <div class="clearfix">
        <a href="Slide3.jpg"><img src="Slide3.jpg" alt="Introduction Image" class="image-left"></a>
        <p>
            The main objective of this research is to detect adversarial examples in DNNs using a method known as feature squeezing. This approach compresses the feature space, making it harder for adversarial perturbations to evade detection, while maintaining the network's overall performance on clean data.
        </p>
    </div>

    <h3>Introduction</h3>
    <div class="clearfix">
        <a href="Slide4.jpg"><img src="Slide4.jpg" alt="Problem Image" class="image-left"></a>
        <p>
            This slide introduces the problem of adversarial examples—inputs that have been deliberately altered to cause deep learning models to misclassify. Adversarial attacks pose a significant threat to the security of machine learning systems, as the changes made to inputs can be nearly imperceptible to humans but catastrophic for models. The growing importance of detecting these attacks is emphasized in high-stakes applications like autonomous driving and cybersecurity.
        </p>
    </div>

    <h3>Adversarial Sample</h3>
    <div class="clearfix">
        <a href="Slide5.jpg"><img src="Slide5.jpg" alt="Solution Image" class="image-left"></a>
        <p>
            An adversarial example is an input designed to deceive a classifier into making incorrect predictions. This slide highlights how small, often imperceptible, alterations to input data can mislead even highly accurate classifiers. Understanding this concept is key to recognizing the vulnerability of neural networks to such attacks.
        </p>
    </div>

    <h3>Types of Adversarial Examples</h3>
    <div class="clearfix">
        <a href="Slide6.jpg"><img src="Slide6.jpg" alt="Residual Example 1" class="image-left"></a>
        <p>
            This slide explains the two primary categories of adversarial examples: targeted, where the attacker forces the model to classify an input as a specific wrong class, and untargeted, where any incorrect classification is the goal. The distinction helps inform different defense strategies.
        </p>
    </div>

    <h3>Metrics for Adversarial Detection</h3>
    <div class="clearfix">
        <a href="Slide8.jpg"><img src="Slide8.jpg" alt="Residual Example 2" class="image-left"></a>
        <p>
            Introduces the distance metrics used to detect adversarial examples. The LP-norm metrics measure the extent of changes between the original and adversarial examples, offering a way to quantify the perturbations. These metrics help in assessing the strength of adversarial attacks and the model's robustness.
        </p>
    </div>

    <h3>Adversarial Attack Methods</h3>
    <div class="clearfix">
        <a href="Slide9.jpg"><img src="Slide9.jpg" alt="Residual Blocks Image" class="image-left"></a>
        <p>
            This slide outlines common methods for generating adversarial examples, such as FGSM, BIM, DeepFool, and CW attacks. These methods represent various ways attackers can craft inputs to exploit vulnerabilities in neural networks.
        </p>
    </div>

    <h3>Motivations</h3>
    <div class="clearfix">
        <a href="Slide10.jpg"><img src="Slide10.jpg" alt="Residual Blocks Image" class="image-left"></a>
        <p>
            The motivations behind feature squeezing as a defense mechanism are outlined here. Traditional adversarial defenses, like adversarial training, can be computationally expensive. Feature squeezing provides a more efficient alternative by reducing the adversary's available search space, combining multiple feature vectors into one sample, and helping detect adversarial examples with minimal overhead.
        </p>
    </div>

    <h3>Feature Squeezers</h3>
    <div class="clearfix">
        <a href="Slide11.jpg"><img src="Slide11.jpg" alt="Residual Network Image" class="image-left"></a>
        <p>
            This slide introduces the core technique of feature squeezers, which apply transformations to reduce the complexity of input features. By combining similar feature vectors, feature squeezing can limit the space adversaries can exploit to craft attacks. This method enhances the ability to detect adversarial examples.
        </p>
    </div>

    <h3>Detection Framework</h3>
    <div class="clearfix">
        <a href="Slide12.jpg"><img src="Slide12.jpg" alt="Shortcut Connection Dimensions Image" class="image-left"></a>
        <p>
            The feature squeezing detection framework includes techniques such as bit depth reduction and spatial smoothing. These transformations make adversarial perturbations more noticeable by limiting the pixel-level variations an attacker can introduce while maintaining input semantics.
        </p>
    </div>

    <h3>Block Diagram</h3>
    <div class="clearfix">
        <a href="Slide13.jpg"><img src="Slide13.jpg" alt="Experiments 1" class="image-left"></a>
        <p>
            This diagram visually represents the feature squeezing detection framework. It provides an overview of how different feature squeezing techniques are applied to detect adversarial examples.
        </p>
    </div>

    <h3>Bit Depth Reduction</h3>
    <div class="clearfix">
        <a href="Slide14.jpg"><img src="Slide14.jpg" alt="Experiments 1" class="image-left"></a>
        <p>
            A core feature squeezing technique, bit depth reduction works by eliminating fine-grained adversarial perturbations while preserving the overall semantic content of the input. This slide shows how reducing bit depth limits the ability of attackers to subtly manipulate images.
        </p>
    </div>

    <h3>Accuracy of Bit Depth Reduction</h3>
    <div class="clearfix">
        <a href="Slide15.jpg"><img src="Slide15.jpg" alt="Experiments 1" class="image-left"></a>
        <p>
            This slide discusses the effectiveness of bit depth reduction in detecting adversarial attacks while maintaining high accuracy for legitimate inputs. It highlights how feature squeezing can enhance model robustness.
        </p>
    </div>

    <h3>Spatial Smoothing</h3>
    <div class="clearfix">
        <a href="Slide17.jpg"><img src="Slide17.jpg" alt="Bottleneck Block Design Image" class="image-left"></a>
        <p>
            Spatial smoothing, another key feature squeezing method, is introduced here. This technique uses filtering (such as median filtering) to reduce noise and smooth out adversarial perturbations, particularly useful in mitigating attacks like salt-and-pepper noise.
        </p>
    </div>

    <h3>Median Filtering</h3>
    <div class="clearfix">
        <a href="Slide18.jpg"><img src="Slide18.jpg" alt="Limitations Image" class="image-left"></a>
        <p>
            This slide provides an explanation of median filtering, a common spatial smoothing technique. By replacing a pixel’s value with the median of its neighboring pixels, this filter helps eliminate noise introduced by adversarial attacks, enhancing image integrity.
        </p>
    </div>

    <h3>Non-Local Means Smoothing</h3>
    <div class="clearfix">
        <a href="Slide20.jpg"><img src="Slide20.jpg" alt="Aftermath Image" class="image-left"></a>
        <p>
            This slide introduces non-local means smoothing, a more advanced spatial smoothing technique. Instead of just considering neighboring pixels, it averages pixels based on overall similarity, offering a more refined approach to mitigating adversarial noise.
        </p>
    </div>

    <h3>Accuracy of Spatial Smoothing</h3>
    <div class="clearfix">
        <a href="Slide23.jpg"><img src="Slide23.jpg" alt="Limitations Image" class="image-left"></a>
        <p>
            The slide evaluates the success of spatial smoothing methods, such as median filtering and non-local means smoothing, in defending against adversarial attacks. It presents data demonstrating the method’s ability to maintain classification accuracy despite adversarial perturbations.
        </p>
    </div>

    <h3>Experimental Setup</h3>
    <div class="clearfix">
        <a href="Slide24.jpg"><img src="Slide24.jpg" alt="Limitations Image" class="image-left"></a>
        <p>
            This slide outlines the experimental design used to test feature squeezing. The datasets (MNIST, CIFAR-10, ImageNet) and adversarial attack methods (FGSM, BIM, DeepFool, JSMA, Carlini-Wagner) are described, along with the models used for testing (7-layer CNN, DenseNet, MobileNet).
        </p>
    </div>

    <h3>Detection Evaluation</h3>
    <div class="clearfix">
        <a href="Slide25.jpg"><img src="Slide25.jpg" alt="Aftermath Image" class="image-left"></a>
        <p>
            This slide evaluates the effectiveness of feature squeezing in mitigating adversarial attacks. Feature squeezing is shown to reverse adversarial perturbations in many cases, ensuring that the classifier’s predictions are not compromised by subtle input alterations.
        </p>
    </div>

    <h3>Adversarial Threat Models</h3>
    <div class="clearfix">
        <a href="Slide26.jpg"><img src="Slide26.jpg" alt="Limitations Image" class="image-left"></a>
        <p>
            The two types of adversarial threat models—oblivious and adaptive adversaries—are explained here. Oblivious adversaries lack knowledge of the detection framework, while adaptive adversaries have full knowledge of both the target model and the detection methods, making them more difficult to defend against.
        </p>
    </div>

    <h3>Detection Success for MNIST</h3>
    <div class="clearfix">
        <a href="Slide27.jpg"><img src="Slide27.jpg" alt="Limitations Image" class="image-left"></a>
        <p>
            This slide presents the results of feature squeezing on detecting adversarial examples in the MNIST dataset, showcasing the success of the technique across different adversarial attacks.
        </p>
    </div>

    <h3>Aggregated Detection Results</h3>
    <div class="clearfix">
        <a href="Slide28.jpg"><img src="Slide28.jpg" alt="Limitations Image" class="image-left"></a>
        <p>
            This slide consolidates the detection results across various datasets and attack types, demonstrating the broad applicability and effectiveness of feature squeezing in mitigating adversarial examples.
        </p>
    </div>

    <h3>Adaptive Adversary Results</h3>
    <div class="clearfix">
        <a href="Slide29.jpg"><img src="Slide29.jpg" alt="Limitations Image" class="image-left"></a>
        <a href="Slide31.jpg"><img src="Slide31.jpg" alt="Aftermath Image" class="image-left"></a>
        <p>
            Here, the success rates of adaptive adversaries, who have full knowledge of both the model and the defense techniques, are analyzed. These adversaries present a tougher challenge, and this slide outlines their success in evading feature squeezing detection.
        </p>
    </div>

    <h3>Randomization as a Defense</h3>
    <div class="clearfix">
        <a href="Slide32.jpg"><img src="Slide32.jpg" alt="Teamwork Acknowledgement Image" class="image-left"></a>
        <a href="Slide33.jpg"><img src="Slide33.jpg" alt="Teamwork Acknowledgement Image" class="image-left"></a>
        <p>
            This slide introduces randomization as a potential solution to counter adaptive adversaries. By adding randomization to the detection process, the model can increase unpredictability, making it harder for adversaries to craft successful attacks.
        </p>
    </div>

    <h3>Limitations</h3>
    <div class="clearfix">
        <a href="Slide34.jpg"><img src="Slide34.jpg" alt="Limitations Image" class="image-left"></a>
        <p>
            This slide outlines the limitations of feature squeezing. The technique has only been tested on MNIST, not on larger, more complex datasets like CIFAR-10 and ImageNet. Additionally, a single threshold is applied across different squeezers, which may limit its adaptability to different attack types. The computational cost can also rise significantly when using advanced models like autoencoders.
        </p>
    </div>

    <h3>Summary</h3>
    <div class="clearfix">
        <a href="Slide35.jpg"><img src="Slide35.jpg" alt="Summary Image" class="image-left"></a>
        <p>
            The slide summarizes the key findings of the presentation. Feature squeezing provides a simple yet effective defense mechanism for detecting adversarial examples in deep neural networks. By reducing the search space available to adversaries and combining feature vectors, feature squeezing offers a robust, computationally efficient way to enhance model security.
        </p>
    </div>

<hr style="border: 1px solid #ccc; margin: 20px 0;">

<h2>Discussions</h2>
<h3>Discussion 1: Do you think this method can produce the same results for real-time adversarial examples?</h3>
<ul>
<li>
    <p>The presenter proposed that you might have to get all preprocessing the same size and choose the best window size before running.</p>
</li>
<li>
    <p>Group 6 suggested that the Gaussian mean smoothing depends a lot on the window size and that finding the perfect window size would be impossible in real time.</p>
</li>
</ul>

<h3>Discussion 2: What are possible applications where the proposed method can be used?</h3>
<ul>
<li>
    <p>Groups 1 and 4 said it could be used for any sort of facial or image software, or to automate driving.</p>
</li>
<li>
    <p>The presenter added that it is used in many audio applications as well as biomedical systems.</p>
</li>
</ul>
<hr style="border: 1px solid #ccc; margin: 20px 0;">

<h2>Questions</h2>
<h3>Q1: Group 6 asked: How does bit reduction guarantee important information is not lost?</h3>
<p>
    Simple smoothing works if the sample is not too complicated. If features are complicated or include multiple classes, problems can happen.
</p>

<h3>Q2: Group 8 asked: Do you have to do it for every data point?</h3>
<p>
    You have to do it for the input change and apply preprocessing to the input, not changing parameters of the deep network.

    In real time, you have to process every frame; is this expensive? In real time it is very expensive, as it is specific to these datasets.
</p>

<h3>Q3: Group 8 asked: Did you come across applying this technique in sequential modeling or signal processing?</h3>
<p>
    In signal processing, you also apply smoothing; signals usually have outliers, so this smoothing works well for this but the names are different.

    Changing this to affect adversarial changes in signals is possible but difficult in terms of recognition; probably won't change accuracy that much. Signal processing is complicated.
</p>

</body>
</html>
