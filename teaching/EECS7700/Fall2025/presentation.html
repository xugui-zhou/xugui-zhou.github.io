<!DOCTYPE html>
<html>
<head>
    <title>EECS 7700  - Schedule</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" title="style1">
    <!-- <link rel="icon" type="image/ico" href="images/favicon.ico"> -->
    <!-- <link href='https://fonts.googleapis.com/css?family=Open+Sans:400' rel='stylesheet' type='text/css'> -->
</head>
<body>

    <ul id="nav">
	    <li><a href="index.html">Home</a></li>
	    <li><a href="schedule.html">Schedule</a></li>
	    <li><a style="color:#232C2D; background:#FFFFFF" href="presentation.html">Presentation</a></li>
	</ul>
    

    <div id="all">
        

        <div id="intro">
            <table width = "100%">
                <!-- <tr>
                    <td>
                        <p align="left" style="font-size:22px">
                            <b>EE/CSC 7700 - Fall 2024 Paper Presentations<br></b>
                        </p>
                        
                        
                    </td>
                </tr> -->

                <tr>
                    <td>
                        <p align="left" style="font-size:18px; background-color: lightblue;">
                            <b>Week 13: Knowledge Integration<br></b>
                        </p>
                        <p>
                            <b>Reading tasks </b> <br>
                            
                            A Semantic Loss Function for Deep Learning with Symbolic Knowledge [<a href="https://arxiv.org/abs/1711.11157" target="_blank"> Link </a>]<br>
                            SpecGuard: Specification Aware Recovery for Robotic Autonomous Vehicles from Physical Attacks  [<a href="https://www.dropbox.com/scl/fi/bw1itoqi03xwe3preeoma/CCS_2024_SpecGuard_Pritam.pdf?rlkey=t0qgyssuaac7ntyzhqfbjjrn2&e=1&st=yy6xv8xf&dl=0" target="_blank"> Link </a>]<br>
                            Informed Machine Learning - A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems [<a href="https://ieeexplore.ieee.org/abstract/document/9429985" target="_blank"> Link </a>]<br>
                        </p>

                        <!-- <p>
                            <b>Blog Post 19: Semantic Losss</b> <br>
                            
                            This paper discusses a method of improving the integration of symbolic logic into a neural network loss function by factoring in an additional "semantic loss function" without modifying the actual model or dataset(s) utilized. The utility of symbolic knowledge is that it improves user interpratability in the loss function without sacrificing too much accuracy, but as neural networks tend to struggle with this information being directly provided, this paper's approach helps alleviate that issue. The concept of semantic loss is described along with explanations of the experimentation on data for semi-supervised classification, as well as more complex scenarios involving reasoning.
                            [<a href="./presentations/BP19/blogpost.html" target="_blank">Read more ...</a>]<br>
                        </p> -->

                         <p>
                            <b>Blog Post 9: Semantic Loss </b> <br>
                            This paper introduces Semantic Loss, a logic-based loss function that allows deep neural networks to respect symbolic constraints during training. Instead of learning only from labeled data, the model is also guided by a logical formula that defines which output configurations are valid. Semantic Loss is defined as the negative log-probability that the network’s prediction satisfies that formula. This makes it possible to enforce rules such as “exactly one label must be true,” “the path must be continuous,” or “rankings must be transitive,” without changing the network architecture. The authors show that adding Semantic Loss improves performance on semi-supervised classification, structured path prediction, and ranking tasks. Overall, the work demonstrates a principled way to combine symbolic reasoning with deep learning by treating logic as a differentiable regularizer.
                            [<a href="presentations/Blogpost-08/blogpost-08.html" target="_blank">Read more ...</a>]<br>
                        </p>
                 
                        <p align="left" style="font-size:18px; background-color: lightblue;">
                            <b>Week 12: ML Interpretebility<br></b>
                        </p>
                        <p>
                            <b>Reading tasks </b> <br>                         
                            A Survey for Machine Learning Security to Securing Machine Learning for CPS [<a href="https://ieeexplore.ieee.org/document/9252851" target="_blank"> Link </a>]<br>
                            Asymmetry Vulnerability and Physical Attacks on Online Map Construction for Autonomous Driving  [<a href="https://arxiv.org/abs/2509.06071" target="_blank"> Link </a>]<br>
                        </p>

                         <p>
                            <b>Blog Post 8:  Online Map Construction </b> <br>
                            This paper reveals a fundamental vulnerability in modern online HD map construction models such as MapTR and VectorMapNet. These models exhibit a strong symmetry bias, often mispredicting asymmetric roads—like forks, merges, or sharp turns—as symmetric or straight. This flaw arises from training data imbalance and architectural design, and it can lead to unsafe planning behaviors such as missed turns or unreachable routes. To exploit this weakness, the authors introduce the first physical-world attacks on online map construction using simple roadside tools: a flashlight for camera blinding and adversarial patches for white-box attacks. Their two-stage framework first detects vulnerable asymmetric scenes and then optimizes roadside attack locations. Experiments on the nuScenes dataset and real-world tests show that these attacks significantly degrade map accuracy and increase unsafe planned trajectories. As a partial defense, the paper proposes fine-tuning with asymmetric data to improve robustness. A key limitation is that the attack is most effective only when the vehicle is near the interference source, and its impact weakens quickly once the vehicle passes it. Despite this, the work demonstrates a critical and previously overlooked safety risk in autonomous driving systems.
                            [<a href="presentations/BlogPost-07/Blog Post_Faiza12 (1).html " target="_blank">Read more ...</a>]<br>
                        </p>
                        <p>
                            <b>Blog Post 7:  Resilient Machine Learning for Networked CPS </b> <br>
                            This paper proposed a systematic view about achieving the resilient CPS system by applying ML algorithms. They also discuss the interaction between the ML and CPS. Also point out that ML also faces vulnerability itself. Many aspects of achieving resilient CPS by using ML and resilient ML model are discussed along with promising future research directions.
                            [<a href="presentations/BlogPost-07/BlogPost-07.html" target="_blank">Read more ...</a>]<br>
                        </p>


                        
                   
                        <p align="left" style="font-size:18px; background-color: lightblue;">
                            <b>Week 11: Reinforcement Learning<br></b>
                        </p>
                        <p>
                            <b>Reading tasks </b> <br>
                            
                            Mastering the game of Go with deep neural networks and tree search  [<a href="https://www.nature.com/articles/nature16961" target="_blank"> Link </a>]<br>
                            Adversarial Policies: Attacking Deep Reinforcement Learning  [<a href="https://openreview.net/pdf?id=HJgEMpVFwB" target="_blank"> Link </a>]<br>
                        </p>

                        <p>
                            <b>Blog Post 6: Attacking Deep Reinforcement Learning </b> <br>
                            The presentation consists of using adversarial policies to attack reinforcement learning. This is applied to a two-player Markov game using multi-Joint in four different scenarios with different goals, such as kick and defend, you shall not pass, Sumo, and 2d Sumo. All were trained with LSTM except You shall not pass which was trained with MLP.  The adversarial policy excelled at the you shall not pass scenario by confusing the victim by curling up in a ball. This unexpected behavior caused the adversarial opponent to win 86% of these scenarios. This demonstrates adversarial policies that can be applied to robotics.
                            [<a href="./presentations/BlogPost-06/BlogPost-06.htm" target="_blank">Read more ...</a>]<br>
                        </p>

                        <p>
                            <b>Blog Post 5: AlphaGo </b> <br>
                            The paper proposes a system that masters the game of Go by combining deep policy and value neural networks with Monte Carlo Tree Search (MCTS), introduced as AlphaGo. First, to imitate strong human moves, a supervised policy network on tens of millions of expert positions is trained, then it is refined by self-play reinforcement learning to maximize win rate. A separate value network learns to predict the eventual winner from a position, which leads to reducing dependence on slow rollout simulations. For leaf evaluation during play, Monte Carlo Tree Search uses the policy network’s move probabilities as priors and the value network, to enable efficient exploration of a vast search space. The single-machine version AlphaGo defeated other Go programs in 494/495 games and also beat European champion, Fan Hui, 0 in an official match with score of 5-0. Ablations demonstrated that combining value estimates with rollouts works best and that policy + value + search is clearly stronger than any component alone. Overall, this paper proved that high-branching domains may be conquered by deep learning combined with guided search, opening the door for AlphaGo Zero and AlphaZero.
                            [<a href="presentations/BlogPost-5/Group6blog.html" target="_blank">Read more ...</a>]<br>
                        </p>




                        <p align="left" style="font-size:18px; background-color: lightblue;">
                            <b>Week 9: Adversarial ML<br></b>
                        </p>
                        <p>
                            <b>Reading tasks </b> <br>
                            
                            L-HAWK: A Controllable Physical Adversarial Patch Against a Long-Distance Target  [<a href="https://www.ndss-symposium.org/wp-content/uploads/2025-26-paper.pdf" target="_blank"> Link </a>]<br>
                            Generative Adversarial Nets [<a href="https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf" target="_blank"> Link </a>]<br>
                        </p>
                        
                        <p>
                            <b>Blog Post 4: L-HAWK </b> <br>
                            L-HAWK: A Controllable Physical Adversarial Patch Against a Long-Distance Target (NDSS 2025) introduces a new kind of laser-triggered physical attack on autonomous vehicles (AVs).
                            Conventional adversarial patches fool AV vision models but are always active and affect every vehicle nearby. L-HAWK overcomes this by using a printed patch that stays harmless until a laser signal activates it, allowing attackers to target a specific vehicle at up to 50 meters.
                            [<a href="./presentations/BlogPost-04/LHAWK BLOG POST_final1.html" target="_blank">Read more ...</a>]<br>
                        </p>

                         <p>
                            <b>Blog Post 3: GAN </b> <br>
                            L-HAWK: A Controllable Physical Adversarial Patch Against a Long-Distance Target (NDSS 2025) introduces a new kind of laser-triggered physical attack on autonomous vehicles (AVs).
                            [<a href="./presentations/BlogPost-3.html" target="_blank">Read more ...</a>]<br>
                        </p>
                        
                        
                                                
         

                        
                        <p align="left" style="font-size:18px; background-color: lightblue;">
                            <b>Week 8: Safety Monitoring in CPS<br></b>
                        </p>
                        <p>
                            <b>Reading tasks </b> <br>                           
                            Attacks against process control systems: risk assessment, detection, and response [<a href="https://dl.acm.org/doi/abs/10.1145/1966913.1966959" target="_blank"> Link </a>]<br>
                            Recovery-Guaranteed Sensor Attack Detection for Cyber-Physical Systems [<a href="https://ieeexplore.ieee.org/abstract/document/11018669" target="_blank"> Link </a>]<br>
                        </p>

                        <p>
                            <b>Blog Post 2: Recovery-Guaranteed Sensor Attack Detection </b> <br>
                            This presentation proposes a recovery-guaranteed sensor attack detection framework for cyber‑physical systems (CPS). Unlike prior work that treats detection and recovery separately, the method co-designs detection thresholds with online recoverability verification, ensuring alarms are raised only when there remains enough time (a window of K steps) to safely recover. The architecture integrates residual calculation, state authentication with bounded error, incremental reachability-based recoverability estimation, and dynamic threshold adjustment that tightens or loosens sensitivity to guarantee recovery while minimizing false alarms. Validation spans vehicle platoon, aircraft pitch, and lane-keeping simulators, plus a physical 4‑wheel testbed, demonstrating zero missed recovery windows across bias, delay, and replay attacks.
                            [<a href="./presentations/BlogPost-2/BlogPost-2.html" target="_blank">Read more ...</a>]<br>
                        </p>

                        <p>
                            <b>Blog Post 1: Attacks against PCS </b> <br>
                            This work presents a unified workflow for securing process control systems that couples 
                            <em>risk assessment</em>, <em>physics-aware detection</em>, and <em>automatic response</em>. 
                            Risk is quantified as expected loss under plausible attack scenarios to prioritize protection of 
                            high-criticality sensors (notably pressure). Detection compares measured outputs with model-based 
                            predictions to form residuals and uses a CUSUM statistic to surface slow, stealthy manipulations. 
                            Upon an alarm, a conservative response replaces compromised measurements with trusted estimates to 
                            keep the plant within constraints until operators intervene.
                            [<a href="./presentations/BlogPost-01/BlogPost-01-embedded-final.html" target="_blank">Read more ...</a>]<br>
                        </p>




                        <p align="left" style="font-size:18px; background-color: lightblue;">
                            <b>Examples: Machine Learning Applications<br></b>
                        </p>
                        <p>
                            <b>Reading tasks </b> <br>
                            
                            Deep Residual Learning for Image Recognition [<a href="https://arxiv.org/abs/1512.03385" target="_blank"> Link </a>]<br>
                            Attention Is All You Need [<a href="https://arxiv.org/abs/1706.03762" target="_blank"> Link </a>]<br>
                        </p>

        
                        <p>
                            <b>Blog Post 2: Transformer </b> <br>
                            This paper introduces a novel sequence transduction model architecture named the Transformer. 
                            This architecture is based solely on attention mechanisms, eliminating the need for recursion and convolution. 
                            The model addresses the limitations of sequence models that rely on recursive processes, which perform poorly in parallelization and computational efficiency for longer sequences.
                            The Transformer adopts an encoder-decoder structure, where the encoder consists of identical layers with multi-head self-attention and fully connected feed-forward networks, 
                            while the decoder mirrors this structure but adds a multi-head attention layer on the encoder's output; utilizing scaled dot-product attention and multi-head attention, 
                            the model computes the importance of key-value pairs based on queries and allows joint attention across different subspaces, 
                            with encoder-decoder attention enabling the decoder to focus on all input positions, 
                            self-attention improving contextual understanding by attending to all positions within layers, 
                            and positional encodings ensuring the model captures the order of tokens in a sequence.
                            [<a href="../Fall2024/presentations/p2/final.html" target="_blank">Read more ...</a>]<br>
                        </p>

                        <p>
                            <b>Blog Post 1: ResNet </b> <br>
                            As the number of layers of neural networks increases, the problems of overfitting, gradient vanishing, and gradient explosion often occur, so this article came into being. In this paper, the concept of deep residual networks (ResNets) is proposed. By introducing "shortcut connections," this study solves the problem of gradient vanishing in deep network training and has an important impact on the field of deep learning. The method of the paper explicitly redefines the network layers as learning residual functions relative to the inputs. By learning residuals, the network can be optimized more easily and can train deeper models more efficiently. Therefore, this method can help solve the performance degradation problem that may occur when the network layer increases. In addition, the article displays the experimental part. The model shows significant improvements in handling large-scale visual recognition tasks like ImageNet and CIFAR-10. The application of deep residual networks in major visual recognition competitions like ILSVRC and COCO 2015 further proves their power and wide applicability.
                            [<a href="../Fall2024/presentations/p1/final.html" target="_blank">Read more ...</a>]<br>
                        </p>
                        <!-- <p align="right" style="font-size:18px">
                         <a href="./presentations/p1/final.html" target="_blank"> Read more ... </a><br>
                        </p> -->
                        
                        
                    </td>
                </tr>

            </table>
        </div>
        <br>
        <br>
    </div>


    <!-- <hr> -->
    </br>

  

</body>
</html>