<!DOCTYPE html>
<html>
<head>
    <title>EECS 7700  - Schedule</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" title="style1">
    <!-- <link rel="icon" type="image/ico" href="images/favicon.ico"> -->
    <!-- <link href='https://fonts.googleapis.com/css?family=Open+Sans:400' rel='stylesheet' type='text/css'> -->
</head>
<body>

    <ul id="nav">
	    <li><a href="index.html">Home</a></li>
	    <li><a href="schedule.html">Schedule</a></li>
	    <li><a style="color:#232C2D; background:#FFFFFF" href="presentation.html">Presentation</a></li>
	</ul>
    

    <div id="all">
        

        <div id="intro">
            <table width = "100%">
                <!-- <tr>
                    <td>
                        <p align="left" style="font-size:22px">
                            <b>EE/CSC 7700 - Fall 2024 Paper Presentations<br></b>
                        </p>
                        
                        
                    </td>
                </tr> -->

                <tr>
                    <td>
                        <p align="left" style="font-size:18px; background-color: lightseagreen;">
                            <b>Module 1: Perception<br></b>
                        </p>
                        <p>
                            <b>Reading tasks </b> <br>
                            
                            Sensor and Sensor Fusion Technology in Autonomous Vehicles: A Review [<a href="https://www.mdpi.com/1424-8220/21/6/2140" target="_blank"> Link </a>]<br>
                            Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks [<a href="https://ieeexplore.ieee.org/document/7485869" target="_blank"> Link </a>]<br>

                            You Only Look Once: Unified, Real-Time Object Detection [<a href="https://www.academis.eu/machine_learning/_downloads/51a67e9194f116abefff5192f683e3d8/yolo.pdf" target="_blank"> Link </a>]<br>
                            DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving [<a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Chen_DeepDriving_Learning_Affordance_ICCV_2015_paper.pdf" target="_blank"> Link </a>]<br>

                            Probabilistic 3D Multi-Modal, Multi-Object Tracking for Autonomous Driving [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561754" target="_blank"> Link </a>]<br>
                            Simple online and realtime tracking with a deep association metric [<a href="https://ieeexplore.ieee.org/abstract/document/8296962" target="_blank"> Link </a>]<br>

                        </p>

                        <p>
                            <b>Blog Post 3: Yolo </b> <br>
                            This paper presents YOLO, a real-time object detection method that achieves better accuracy with object detection than comparable real-time object detection methods. The authors found that when YOLO is used in conjunction with Faster CNN, it achieves better accuracy than without its implementation.
                            [<a href="./presentations/blog post 3/You Only Look Once Unified, Real-Time Object Detection.html" target="_blank">Read more ...</a>]<br>
                        </p>

                        <p>
                            <b>Blog Post 2: Sensor Fusion </b> <br>
                            The paper "Sensor and Sensor Fusion Technology in Autonomous Vehicles: A Review" provides a comprehensive overview of the role of sensors in autonomous vehicles (AVs), emphasizing their importance in perception, localization, and decision-making. It examines key sensor technologies such as cameras, LiDAR, and radar, discussing their strengths, limitations, and performance under various environmental conditions. The paper highlights the necessity of sensor calibration as a prerequisite for accurate data fusion and object detection, reviewing available open-source calibration tools. Additionally, it categorizes sensor fusion approaches into high-level, mid-level, and low-level fusion, evaluating state-of-the-art algorithms that enhance object detection and overall driving safety. The review concludes by addressing challenges in sensor fusion, such as data synchronization and environmental adaptability, while proposing future research directions for improving autonomous vehicle technology.
                            [<a href="./presentations/Blog Post 2/blog.html" target="_blank">Read more ...</a>]<br>
                        </p>
                        <p>
                            <b>Blog Post 1: Faster R-CNN </b> <br>
                            The paper introduces Faster R-CNN, a deep learning-based object detection framework that improves upon previous region-based detection models by integrating a Region Proposal Network (RPN). Unlike earlier methods that relied on computationally expensive region proposal algorithms, Faster R-CNN shares convolutional features between region proposal and object detection networks, making the process nearly cost-free. The RPN generates region proposals efficiently, which are then refined by the Fast R-CNN detector. The experimental results demonstrate that Faster R-CNN significantly improves detection accuracy while achieving real-time processing speeds, making it a powerful tool for object detection tasks.
                            [<a href="./presentations/Blog Post 1/output.html" target="_blank">Read more ...</a>]<br>
                        </p>
                        
                        
                        <p align="left" style="font-size:18px; background-color: lightseagreen;">
                            <b>Example: Machine Learning Applications<br></b>
                        </p>
                        <p>
                            <b>Reading tasks </b> <br>
                            
                            Deep Residual Learning for Image Recognition [<a href="https://arxiv.org/abs/1512.03385" target="_blank"> Link </a>]<br>
                            Attention Is All You Need [<a href="https://arxiv.org/abs/1706.03762" target="_blank"> Link </a>]<br>
                        </p>

                        
                        <p>
                            <b>Blog Post Example: ResNet </b> <br>
                            As the number of layers of neural networks increases, the problems of overfitting, gradient vanishing, and gradient explosion often occur, so this article came into being. In this paper, the concept of deep residual networks (ResNets) is proposed. By introducing "shortcut connections," this study solves the problem of gradient vanishing in deep network training and has an important impact on the field of deep learning. The method of the paper explicitly redefines the network layers as learning residual functions relative to the inputs. By learning residuals, the network can be optimized more easily and can train deeper models more efficiently. Therefore, this method can help solve the performance degradation problem that may occur when the network layer increases. In addition, the article displays the experimental part. The model shows significant improvements in handling large-scale visual recognition tasks like ImageNet and CIFAR-10. The application of deep residual networks in major visual recognition competitions like ILSVRC and COCO 2015 further proves their power and wide applicability.
                            [<a href="./presentations/p1/final.html" target="_blank">Read more ...</a>]<br>
                        </p>
                        <!-- <p align="right" style="font-size:18px">
                         <a href="./presentations/p1/final.html" target="_blank"> Read more ... </a><br>
                        </p> -->
                        
                        
                    </td>
                </tr>

            </table>
        </div>
        <br>
        <br>
    </div>


    <!-- <hr> -->
    </br>

  

</body>
</html>