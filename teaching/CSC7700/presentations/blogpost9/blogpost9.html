<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Home - Simple Blog Template</title>

    <style>
      .image-left {
        float: left;
        margin-right: 20px;
        margin-bottom: 20px;
        width: 300px;
      }
      .clearfix::after {
        content: "";
        clear: both;
        display: table;
      }
    </style>

    <!-- Bootstrap Core CSS -->
    <link href="blogpost9_files/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="blogpost9_files/simple-blog-template.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

  </head>

  <body>

    <!-- Page Content -->
    <div class="container">

      <div class="row">

        <!-- Blog Entries Column -->
        <div class="col-md-12">
          <h2 class="post-title" style="font-size:30px; font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            <a>Deep Reinforcement Learning for Autonomous Driving: A Survey</a>
          </h2>
          <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Paper written by B. Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A. Al Sallab, and Senthil Yogamani
          </p>
          <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Presented by Sujan Gyawali
          </p>
          <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Blog post by George Hendrick
          </p>
          <p style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif"><span class="glyphicon glyphicon-time"></span> Presented on April 10, 2025</p>
          <a class="btn btn-default" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif" href="https://ieeexplore.ieee.org/document/9351818">Paper Link</a>

          <br><br><hr>

          <!-- Brief Summary Section -->
          <p class="lead" style="font-size:24px; font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Brief Summary:
          </p>
          <p style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Sujan Gyawali presents the paper Deep Reinforcement Learning for Autonomous Driving: A Survey. The paper summarizes deep reinforcement learning 
            algorithms and provides a taxonomy of automated driving tasks where these methods have been employed. The paper discusses the role of 
            simulators in training agents, as well as methods to validate, test, and robustify existing solutions in RL.
          </p>

          <!-- Slides Section -->
          <p class="lead" style="font-size:24px; font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Slides:
          </p>
          <!-- Slide 1 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/1.png">
              <img src="blogpost9_files/1.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              Title slide giving credit to the authors.
            </p>
          </div>
          <!-- Slide 2 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/2.png">
              <img src="blogpost9_files/2.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              Reinforcement learning involves an agent learning by doing actions and receiving feedback. Deep reinforcement learning uses deep neural networks to 
              assist the agent in understanding complex environments.
            </p>
          </div>
          <!-- Slide 3 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/3.png">
              <img src="blogpost9_files/3.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              Autonomous driving systems involve components such as cameras, LiDAR, RADAR, and Ultrasonic sensors which are used to capture raw, real time data. 
              Sensors are placed in a configuration that allows a 360 degree view, and these sensors provide critical information. The data from the sensors is then 
              transformed into a structured view of the surroundings.
            </p>
          </div>
          <!-- Slide 4 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/4.png">
              <img src="blogpost9_files/4.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              Sensor data is fused into a unified object map such as a bird's eye view. Behavior prediction can then be performed on dynamic objects such as cars or 
              pedestrians. It handles sensor noise and uncertainty using probabilistic fusion techniques. Path and motion planning can then be determined such that 
              safe and feasible routes can be created. Trajectory optimization is used to ensure smooth efficient driving and a driving policy is determined.
            </p>
          </div>
          <!-- Slide 5 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/5.png">
              <img src="blogpost9_files/5.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              Graphic outlining the different components of RL systems.
            </p>
          </div>
          <!-- Slide 6 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/6.png">
              <img src="blogpost9_files/6.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
             Introduces value based methods such as Q-Learning, which learns the Q-function and follows the best possible policy, and Deep Q-Networks, which combines 
             Q-Learning with Deep Neural Networks to handle complex, high-dimensional input spaces.
            </p>
          </div>
          <!-- Slide 7 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/7.png">
              <img src="blogpost9_files/7.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              Policy based methods involve learning the policy directly, instead of learning value functions like Q-values. The policy tells the agent what to take 
              in each state. The policy is usually represented by a neural network and policy gradients are used to improve the policy step by step. Actor-critic methods 
              is a combination of policy-based and value-based methods. The actor determines what action to take while the critic evaluates how much value the action had.
            </p>
          </div>
          <!-- Slide 8 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/8.png">
              <img src="blogpost9_files/8.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              In model-based, the agent learns how the environment works and builds a model for how actions lead to rewards. Actions are planned and simulated before they 
              are actually attempted, which helps reduce real-world interactions. In model-free, the agent instead lears from trial and error by interacting with the 
              environment and collecting rewards.
            </p>
          </div>
          <!-- Slide 9 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/9.png">
              <img src="blogpost9_files/9.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              On-policy methods involve the agent learning from their current behavior, improving the same policy it's using to act. Off-policy involes the agent learning 
              from past behavior or from another policy.
            </p>
          </div>
          <!-- Slide 10 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/10.png">
              <img src="blogpost9_files/10.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              In basic RL, values are stored for each state-action pair in table, which can cause issues in real-world problems when the number of states and actions is too 
              large. DRL solves this using deep neural networks to learn from complex inputs, which helps handle large or continuous environments.
            </p>
          </div>
          <!-- Slide 11 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/11.png">
              <img src="blogpost9_files/11.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              Extensions to RL include reward shaping, a technique which guides learning by adding extra rewards, and multiple-agent reinforcement learning, which involves 
              multiple RL agents learning and acting in the same environment.
            </p>
          </div>
          <!-- Slide 12 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/12.png">
              <img src="blogpost9_files/12.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              Multiple-object reinforcement learning handles situations where an agent must balance multiple goals at the same time, while learning from demonstrations allows 
              an agent to learn how to act by copying examples from an expert.
            </p>
          </div>
          <!-- Slide 13 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/13.png">
              <img src="blogpost9_files/13.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              State Space, Action Space, and Reward Functions are used to train self-driving cars using deep reinforcement learning. State Space involves what the car knows, 
              Action Space gives information about the car's capabilities, and the Reward Function guides learning. Motion planning and trajectory optimization are applied 
              to further determine a feasible path and consider static and dynamic elements.
            </p>
          </div>
          <!-- Slide 14 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/14.png">
              <img src="blogpost9_files/14.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              Simulators are used with RL to train and test driving agents safely by simulating vehicle dynamics, sensors, environments, and state-action pairs.
            </p>
          </div>
          <!-- Slide 15 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/15.png">
              <img src="blogpost9_files/15.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              Validating RL systems is difficult as different implementations, hyperparameters, and evaluations setups can lead to inconsistent results. It's difficult to 
              know if a policy is truly effective or just overfitting to a specific setup. Simulators are used to improve safety as well as offering cheap, safe, and labeled 
              data for training RL models.
            </p>
          </div>
          <!-- Slide 16 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/16.png">
              <img src="blogpost9_files/16.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              RL agents need a huge number of trials to learn good policies, and reward shaping helps the agent learn faster by giving more frequent feedback. Intrinsic reward 
              functions can help guide learning based on curiosity and prediction errors. The agent rewards itself for exploring new or surprising situations.
            </p>
          </div>
          <!-- Slide 17 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/17.png">
              <img src="blogpost9_files/17.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              Imitation learning uses expert demonstrations to train an agent, but the expert may not visit all possible states the agent might face. DAgger helps by letting 
              the agent act, then asking the expert to label those new states so the training set includes both expert and agent experiences. Likewise, MARL helps assist vehicles 
              coordinate in complex situations.
            </p>
          </div>
          <!-- Slide 18 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/18.png">
              <img src="blogpost9_files/18.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              Agents have to learn safety, especially when handling unpredictable traffic or pedestrians. Safe DAgger adds a safety policy to predict mistakes of the main policy 
              by avoiding unsafe actions without constantly relying on expert feedback.
            </p>
          </div>
          <!-- Slide 19 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/19.png">
              <img src="blogpost9_files/19.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              RL is still a growing field in autonomous driving and is limited to public datasets and practical guidance. This paper organizes and reviews key RL methods and 
              challenges in driving tasks. There are many challenges such as sample efficiency, safety, and bridging the gap between simulation and reality.
            </p>
          </div>
          <!-- Slide 20 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/20.png">
              <img src="blogpost9_files/20.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              Sujan believes combining trial-and-error learning with deep neural networks can help self-driving cars learn complex tasks. The paper had comprehensive coverage 
              as well as a real world focus, however the paper had limited experimental results as well as heavy on technical language.
            </p>
          </div>
          <!-- Slide 21 -->
          <div class="clearfix">
            <a href="https://xugui-zhou.github.io/teaching/CSC7700/presentations/blog%20post%203/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection/21.png">
              <img src="blogpost9_files/21.png" class="image-left"></a>
            <p class="lead" style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
              Later work involved a survey which focused on the latest advancements in behavior planning using reinforcement learning and another review on safe reinforcement 
              learning for autonomous driving.
            </p>
          </div>
          
          <!-- Q&A Section -->
          <p class="lead" style="font-size:24px; font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Q&amp;A:
          </p>
          <p style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Question: Aleksander asked what is the difference between an agent and a model in this context?
          </p>
          <p style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Answer: The presenter responded that the agent is the entity that interacts with the environment, observes it, and performs actions 
            to maximize its reward. The model is the representation of the environment the agent uses for learning and planning.
          </p>

          <!-- Discussion Section -->
          <p class="lead" style="font-size:24px; font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Discussion:
          </p>

          <p style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Question 1: In a future city full of driverless cars, how do you think they'll "talk" to each other? Could multi-agent RL by the key to traffic harmony or chaos?
          </p>
          <p style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Aleksandar: Asserted belief of the importance to optimize traffic, and agrees multi-agent RL could be effective.
          </p>
          <p style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Professor: Added that there are public concerns about using reinforcement learning. The goal is to reach optimization to minimize cost and improve efficiency and time, 
            as well as reduct cost, but reinforcement learning needs a long time to optimize the process.
          </p>

          <p style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Question 2: Would you prefer a car that learns by exploring the world or one that builds a mental model first? Why might that choice matter in real-time traffic.
          </p>
          <p style="font-family:'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif">
            Aleksandar: Agrees that real world learning could cause dangerous consequences.
          </p>

          <hr>

          <br><br><br>

        </div>

      </div>
      <!-- /.row -->

    </div>
    <!-- /.container -->

    <!-- jQuery -->
    <script src="blogpost2_files/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="blogpost2_files/bootstrap.min.js"></script>

  


</body></html>